\documentclass[12pt]{article}
\author{Utkarsh Patange 11493}
\title{Lovasz Local Lemma: Proof and Applications}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[lined,boxed,linesnumbered]{algorithm2e}
\usepackage{algorithmicx}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}


\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}



\begin{document}
\maketitle
\section{Lemma statement}
The asymmetric lovasz local lemma \cite{original} states the following:
\begin{theorem}
\label{lll}
 Let $\mathbb{A}$ be a finite set of events in a probability space. For $A\in \mathbb{A}$ let $\Gamma(A)\subset \mathbb{A} $ such that $A$ is independent from the collection of events $A\setminus (\{A\}\cup\Gamma(A) ) $. If $\exists x:\mathbb{A} \longrightarrow (0,1): $
 \[
    \forall A\in \mathbb{A} : \Pr[A] \leq x(A) \prod\limits_{B\in\Gamma(A) }(1-x(B) )
 \]
 then
 \[
    \Pr(\cap_{A\in\mathbb{A}}\overline{A} ) \geq \prod\limits_{A\in\mathbb{A}}(1-x(A) )
 \]
 In particular, the probability is positive
\end{theorem}
First we study the non-constructive proof of theorem \ref{lll} given in \cite{assym}

\section[Proof]{Non-constructive Proof\footnote{The same proof has been added by me in the wikipedia article of Lovasz Local Lemma (\text{https://en.wikipedia.org/wiki/Lovasz\_Local\_Lemma}) under the username Uspatange on $31^{st}$ March 2014 }}
We prove the asymmetric version of the Lemma. By using the principle of mathematical induction we prove that $ \Pr\left(A|\cap_{B \in S}\overline{B}\right)\leq x\left(A\right) \forall S \subset \mathcal{A}, A \not\in S $. The induction here is applied on the size (cardinality) of the set $ S $. For base case $S=\phi$ the statement obviously holds since $ \Pr\left(A_i\right) \leq x\left(A_i\right) $. We need to show that the inequality holds for any subset of $\mathcal{A} $ of a certain cardinality given that it holds for all subsets of a lower cardinality.

Let $S_{1} = S\cap \Gamma(A), S_{2} = {{S \smallsetminus S_{1}}}$. We have from Bayes' theorem

\[
\Pr\left(A|\cap_{B\in S} \overline{B}\right) = \frac{\Pr\left(A\cap\cap_{B\in S_{1}} \overline{B}| \cap_{B\in S_{2}} \overline{B}\right)}{\Pr\left(\cap_{B\in S_{1}}\overline{B}|\cap_{B\in S_{2}} \overline{B} \right)}  \]

We bound the numerator and denominator of the above expression separately. For this, let $ S_{1}=\{B_{j1},B_{j2},\ldots,B_{jl}\} $. First, exploiting the fact that $A$ does not depend upon any event in $ S_{2} $.

\begin{equation}
 Numerator \leq \Pr\left(A|\cap_{B\in S_{2}} \overline{B}\right) = \Pr(A) \leq x(A) \prod\limits_{B\in\Gamma(A)}\left(1-x(B)\right)
 \label{numer}
\end{equation}



Expanding the denominator by using Bayes' theorem and then using the inductive assumption, we get

\begin{equation}
\label{denom}
\begin{split}
 
 Denominator & = \Pr\left(\overline{B}_{j1}|\cap_{t=2}^l \overline{B}_{jt}\cap_{B\in S_{2}} \overline{B} \right)\cdot \Pr\left(\overline{B}_{j2}|\cap_{t=3}^l\overline{B}_{jt}\cap_{B\in S_{2}} \overline{B} \right)\cdot \ldots \\
 &\ldots \cdot \Pr\left(\overline{B}_{jl}|\cap_{B\in S_{2}} \overline{B} \right)  \geq \prod\limits_{B\in S_{1}} \left(1-x(B)\right) 
\end{split}

\end{equation}


The inductive assumption can be applied here since each event is conditioned on lesser number of other events, i.e. on a subset of cardinality less than $|S|$. From equations and (2), we get

\[ 
\Pr\left(A|\cap_{B\in S} \overline{B}\right) \leq x(A)\prod\limits_{B\in \Gamma(A)-S_{1}}(1-x(B)) \leq x(A)  \]

Since value of x is always in (0,1). Note that we have essentially proved $ \Pr\left(\overline{A}|\cap_{B\in S} \overline{B}\right) \geq 1-x(A) $. To get the desired probability, we write it in terms of conditional probabilities applying Bayes' theorem repeatedly. Hence,

\[ 
\Pr\left(\overline{A_1} \cap \ldots \cap \overline{A_n} \right) = \Pr\left(\overline{A_1}|\overline{A_{2}}\cap \ldots \overline{A_{n}}\right)\cdot\Pr\left(\overline{A_2}|\overline{A_{3}}\cap \ldots \overline{A_{n}}\right)\cdot \ldots \cdot \Pr\left(\overline{A_{n}}\right) \leq \prod\limits_{A\in\mathcal{A}}(1-x(A)) 
\]
Which is what we had intended to prove.

Here we study a constructive proof for theorem \ref{lll} given in \cite{constructive}.
\section{Assumptions and Terms Used}
To get an algorithmic access to the problem, we assume that all the events can be determined by some subset of a finite collection of mutually independent random variables in a fixed probability space. This indeed is the case in almost all known applications of the Lovasz Local Lemma.

Terms used:
\begin{enumerate} \itemsep -2pt
 \item $\Omega :=$ A fixed probability space
 \item $\mathbb{P}$ := Set of mutually independent random variables in $\Omega$
 \item $\mathbb{A} :=$ Set of events determined by some $S\subseteq\mathbb{P} $
 \item An evaluation of variables in S \emph{violates} $A\in\mathbb{A} $ if it makes $A$ happen
 \item $vbl(A) :=$ The unique minimal $S\subseteq\mathbb{P} $ that determines $A$
 \item $G_\mathbb{A}(Dependency\,\, Graph) := (\mathbb{A},E) $ where $E$ is a set of undirected edges and $(A,B)\in E $ if $A\neq B, vbl(A)\cap vbl(B) \neq \phi $
 \item $\Gamma(A):=\Gamma_\mathbb{A}(A) $ is the neighborhood of $A$ in $G$
 \item $\Gamma^+(A):=\Gamma(A)\cup\{A\} $
\end{enumerate}
Given the family $\mathbb{A}$, we need to find an algorithm to efficiently find such an evaluation that does not violate any of the events $A\in\mathbb{A}$

\section{Pseudo Code}
\begin{algorithm}
\label{algo}
\For  {$P \in \mathbb{P}$} {
  $v_P \leftarrow$ a random evaluation of $P$ }
\While {$\exists A \in \mathbb{A} : A$ is violated when $(P = v_P : \forall P \in \mathbb{P})$} {
  pick an arbitrary violated event $A \in \mathbb{A}$\\
    \For{$P \in vbl(A)$}  {
    $v_P \leftarrow $ a new random evaluation of $P$
    }
  }
return ($v_P )_{P\in \mathbb{P}}$

 \caption{lllfind}
\end{algorithm}
It is obvious that if algorithm \ref{algo} terminates, it would have definitely found the required evaluation of the random variables. We need to prove that this algorithm not only terminates, but it terminates quickly i.e. there is some bound on the number of iterations.

We will prove that the randomized algorithm \ref{algo} resamples and event $A \in \mathbb{A}$ at most an expected $x(A)/(1-x(A) ) $ times before it terminates. For this purpose, we use the concept of Witness Trees and Execution Logs.

\section{Execution Logs and Witness Trees}
Let's say we fix the method to choose the next event that is to be resampled among all of the events that are violated during an iteration. Let the sequence in which we resample the events be $C$ called an execution log. That is, $C$ is a sequence in which the $t^{th}$ entry is the event that is sampled in the $t^{th} $ step. Thus $C$ now depends entirely upon the random evaluations of the different random variables in $\mathbb{P}$.

A witness tree $\tau$ is a finite rooted tree each of whose vertices are labeled by an event from $\mathbb{A}$. We will denote label of a vertex $v$ by $[v] $ and the set of vertices by $V(\tau)$. Each children of a vertex $u$ must receive labels from $\Gamma^+([u] ) $. We call $\tau$ a proper witness tree if all siblings have distinct labels.

We associate a witness tree with each step of the log. The intuition is that the witness tree will justify the resampling done in that step. For a log $C$ we define $\tau_C(t) $ for each step $t$ in $C$. We will construct $\tau_C(t) $ incrementally as follows:
\begin{algorithm}
\label{construct}
  $\tau_C^{(t)}(t) \leftarrow $  an isolated vertex labeled $C(t)$ \\
  \For {$i\leftarrow t-1$ to 1} {
     $\rho_i \leftarrow \{v\in \tau_C^{(i+1)}(t)| C(i)\in \Gamma^+([v] )\} $ \\
     $\tau_C^{(i+1)}=\tau_C^{(i)} $ \\
     \If {$\rho_i <> \phi$} {
     $v \leftarrow$ a vertex in $\rho_i$ that is farthest from root\\
     $u \leftarrow$ new vertex \\
     $[u] \leftarrow C(i)$ \\
     $child_{i+1}(v) \leftarrow child_i(v)\cup u$
     }
  $\tau_C(t)\leftarrow \tau_C^{(1)}(t) $\\
  return $\tau_C(t)$
  }
\caption{Construct ($C,t$)}
\end{algorithm}
Due to the second step in the construction, we can say that no two vertices having labels that are dependent on each other can ever be siblings. This also proves that any witness tree $\tau_C(t)$ will be proper for any log $C$ and step $t$ in $C$. 

We say that a witness tree $\tau$ appears in a log $C$ if for some step $t$ $\tau=\tau_C(t)$.

\begin{theorem}
 Let $\tau$ be a fixed witness tree and  C the (random) log produced by algorithm \ref{algo}. Then,
 \begin{enumerate}
  \item If $\tau$ appears in C, then $\tau $ is proper
  \item The probability that $\tau$ appears in C is at most $\prod\limits_{v\in V(\tau)}\Pr([v]) $
 \end{enumerate}
\end{theorem}

\proof For a vertex $v\in  V(\tau )$ let $d(v)$ denote the depth of vertex $v$, that is its distance from
the root and let $q(v)$ stand for the step of the algorithm \ref{construct} constructing $\tau_C(t)$ in which $v$ was attached. That is, $q(v)$ is the largest value $q$ with $v$ contained in $V(\tau_C^{(q)}(t) )$


\bibliographystyle{alpha} 
\bibliography{refer.bib}
\end{document}