
\section{A Wiring Problem}
This problem and its solution is given in \cite{random} and is a great example of how randomized rounding can be applied to solve problems.
\subsection{Problem Statement}
There is a $\sqrt{n} \times \sqrt{n}$ grid of unit squares which needs to be wired. We are given some wires and the squares they should connect. The wires can only turn at right angle and at most once. At all times, the wires run parallel to the grid boundaries. So, the wires will have to pass boundaries between adjacent squares to go to the destination. Let $w$ be the maximum number of wires that pass through a certain boundary. The problem is to minimize $w$.

First we convert it into an equivalent Integer Linear Programming (ILP) Problem
\subsection{Equivalent ILP Problem}
Since only a single right turn is allowed, there may be a maximum of two routes possible for each wire to go from one square to another. We use the variables $x_{i0} $ and $x_{i1}$ to denote which route we chose. $x_{i0} = 1$ if we go horizontally first, and $0$ otherwise. $x_{i1} = 1 $ if $x_{i0} = 0$. On fixing the values of $x_{i0}$ and $x_{i1}$, we obviously know which boundaries the $i^{th}$ wire will pass. So we introduce sets for each boundary $b$:

$ T_{b0} = \{i |$ net $i$ passes through $b$ if $x_{i0} = 1 \} $

$ T_{b1} = \{i |$ net $i$ passes through $b$ if $x_{i1} = 1 \}$ \\


Thus, we get the following ILP:

\[
\begin{array}{cccl}
$Minimize$ & w & & \\
$Subject to,$ & x_{i0}  + x_{i1} & = & 1 \,  \forall i\\
 &			x_{i0} , x_{i1} & \in & \{0,1\} \forall i \\
 &			\sum\limits_{i \in T_{b0}} x_{i0} + \sum\limits_{i \in T_{b1}} x_{i1} & \leq & w \, \forall b \\

\end{array}
\]
To get a relaxed version which can be solved in polynomial time, we replace the integer constraints by $x_{i0},x_{i1} \in [0,1]$. But, now we need to map this to the original problem.

\subsection{Mapping LP solution to Original Problem }
Since the LP problem is a relaxed version of the ILP one, we have: $w_{LP} \leq w_{ILP}$. After mapping the solution for the LP to the original problem, let the reported answer be $w_{rep}$. This can be found out from the wiring generated by the mapped solution. We also need to prove that $w_{rep}$ is provably not very bigger than $w_{ILP}$.

Applying randomized rounding, we report $x_{ij,rep} = 1$ with probability $x_{LP}\, \forall i$ and $\forall j \in\{0,1\}$ and 0 otherwise. This ensures that if $x_{ij}$ is close to 0 for some $i$ and $j$, there will be high probability for it to be reported as 0.

Let $w_{rep}(b)$ denote the number of wires through a boundary $b$. By definition, we have:
\[
	\begin{array}{rcl}
	w_{rep}(b) & = & \sum\limits_{i \in T_{b0}} x_{i0,rep} + \sum\limits_{i \in T_{b1}} x_{i1,rep}\\
	E[w_{rep}(b)] & = & \sum\limits_{i \in T_{b0}} E[x_{i0,rep}] + \sum\limits_{i \in T_{b1}} E[x_{i1,rep}] \\
	& = & \sum\limits_{i \in T_{b0}} x_{i0,LP} + \sum\limits_{i \in T_{b1}} x_{i1,LP} \\
	& \leq & w_{LP}\\
	\end{array}
\]
Since all $ x_{ij,rep}$ are independent Bernoulli random variables (with probability $ x_{i0,LP}$ each), we can apply Chernoff Bound. Thus, we get:

\[
	\begin{array}{rcl}
	Pr(w_{rep}(b)\geq (1+\delta)w_{LP}) & \leq & Pr(w_{rep}(b) \geq (1+\delta) \mu)\\
	& \leq & \left(\frac{e^\delta}{(1+\delta)^{(1+\delta)}}\right)^\mu\\
	\end{array}
\]

Now, we have proved that $w_{rep}(b)$ probabilistically very close to $w_{LP}$. By union theorem and the fact that there are less than $2n$ boundaries to a $\sqrt{n} \times \sqrt{n}$ grid, we get: $Pr(w_{rep}\geq (1+\delta)w_{LP}) \leq 2n\left(\frac{e^\delta}{(1+\delta)^{(1+\delta)}}\right)^{w_{LP}}$. and since $w_{LP} \leq w_{ILP}$
\[
	\begin{array}{lrcl}
	\multicolumn{2}{r}{Pr(w_{rep}\geq (1+\delta)w_{ILP})  \, \, \leq   Pr(w_{rep}\geq (1+\delta)w_{LP})} 
	& \leq & 2n\left(\frac{e^\delta}{(1+\delta)^{(1+\delta)}}\right)^{w_{LP}} \\
	$Taking $\delta = 1 & Pr(w_{rep}\geq 2w_{ILP})& \leq & 2n\left(\frac{e}{2^{2}}\right)^{w_{LP}} \\
	$For $w_{LP} = c\log n $ it becomes,$  & Pr(w_{rep}\geq 2w_{ILP}) & \leq & 2n \times n^{-c}\\
	$But for constant $w_{LP}, $say 20$, & Pr(w_{rep}\geq (1+\delta)\times 20) & \leq & 2n\left(\frac{e^{20\delta}}{(1+\delta)^{20(1+\delta)}}\right) \\
$We need $\delta \geq 2e-1 & & \leq & 2ne^{-c \delta^2/4} \\
\multicolumn{2}{l}{$We need $\delta = \Omega(\sqrt{\log n})$ for similar bound. Here the probability$} & \leq & 2n\times n^{-k}\\
\end{array} 
\]

Thus, we have achieved a tight upper bound on the probability that our reported answer to the wiring problem using randomized rounding is not very different from the actual answer. It must be noted, however, that the tightness of the bound depends upon the actual answer $w_{LP}$. If $w_{LP}$ is higher the bound is tighter as opposed to when $w_{LP}$ is low.
\section{Minimum Cut about Two Points}
The cut defined here is different from the cut defined in flow related problems. This problem is an excellent example in which the randomized solution is, not approximately, but exactly equal to the actual solution. This appeared in \cite{srinivasan}
\subsection{Problem Statement}
Given an undirected weighted graph $G$ and two special vertices $s$ and $t$, the problem is to cut
the graph such that $s$ and $t$ are not connected after the cut. Of all the possible such cuts, we need to
minimize the total weight of the edges in the cut. We will first design an Integer Linear Programming problem and solve its relaxed version (which will be a linear programming problem and has polynomial 
time algorithms). This solution will then be mapped to the original problem using randomized rounding.

\subsection{Equivalent ILP Problem}
Given a cut, We take indicator variables $x_i$ for each vertex setting $x_i=0$ if the vertex is reachable from $s$ and 1 otherwise. Hence $x_s=0, x_t=1$. $z_{ij}$ is an indicator variable for the edges indicating whether the edge crosses the cut or not.
The equivalent integer linear programming problem can be stated as follows:
\[
\begin{array}{cccl}
$Minimize$ & \multicolumn{3}{c}{\sum\limits_{0\leq i,j \leq n} c_{ij}z_{ij}}\\
$Subject to,$ & z_{ij} & \geq & x_i - x_j \\
 &			z_{ij} & \geq & x_j - x_j \\
 &			x_s & = & 0 \\
 &			x_t & = & 1 \\
 &			x_i & \in & \{0,1\} \, \forall i \\
 &			z_{ij} & \in & \{0,1\} \, \forall i \forall j
\end{array}
\]
It can be easily seen that for any optimal solution, $z_{ij} = |x_i - x_j|$ will hold. To solve this ILP problem, we relax the integer constraints and solve the LP problem by known polynomial time algorithms. The LP solutions will then have to be mapped to integer feasible solutions (following the constraints) to find the solution to the original problem

\subsection{Mapping LP solutions to Original Problem}
Relaxing the integer constraints to $x_i \in [0,1]$ and $z_{ij} \in [0,1]$, let us say we get the solution $x_i^*$ and $z_{ij}^*$ with objective function value $y*$. Obviously, $y^* \leq y $ where $y$ is the optimum solution to the original problem.

To map the ILP solution back to the original problem by randomized rounding, we choose a number $u \in [0,1)$ randomly uniformly. We report $x_i = 0 $ if $x_i^* \leq u$ and $x_i = 1$ otherwise. Even in this linear programming problem, in an optimum solution $z_{ij}^* = |x_i^* - x_j^*|$ will hold. But, to make the mapped solution feasible in the original problem instance, we will report $z_{ij} = |x_i - x_j|$. Now, $z_{ij}$ will be 1 if $u \in [min(x_i,x_j), max(x_i,x_j)]$. Thus, E($z_{ij}$) = $|x_i^* - x_j^*| = z_{ij}^*$.

Now, the reported answer to the original problem is
$
y = \sum\limits_{0\leq i,j \leq n} c_{ij}z_{ij}
$.
We can find the expected value of the above expression by linearity of expectation:
\[
\begin{array}{rcl}
E[y] & = & E[\sum\limits_{0\leq i,j \leq n} c_{ij}z_{ij}]\\
 & = & \sum\limits_{0\leq i,j \leq n} c_{ij}E[z_{ij}] \\
&	=	& \sum\limits_{0\leq i,j \leq n} c_{ij}z_{ij}^* \\
&	=	& y^*
\end{array}
\]
This mapped solution follows all the constraints of the original ILP problem. Since answer to the original problem must have objective function value $OPT \geq y^*$. Since $E[y] = y^*$, by pigeon-hole principle we must have $y \leq y^*$ for some $u \in [0,1)$. But $OPT \leq y$ since $OPT$ is, after all the optimum objective function value. Hence, we get $y = OPT = y^*$.

This is an example where the solution to the relaxed version of the ILP problem is also the actual solution to the same problem. Hence, we don't need to come up with any probability bounds on the answer.

\section{Maximum Satisfiability}
This is a problem where we apply two approaches to find an approximate solution to a given problem and then merge the two solutions probabilistically to get a better approximation to the optimal solution. This problem is given in \cite{srinivasan}
\subsection{Problem Statement}
Given $m$ clauses in $n$ variables and weights $w_i$ associated with each clause, we need to maximize the sum of weights of satisfied clauses.

Clearly, this is an NP-hard problem since SAT can be reduced to Max-SAT. All we have to do is to find the solution to Max-SAT (with $w_i = 1\forall i\in [n]$ and check if this satisfies all the clauses.

\subsection{Applying Randomized Rounding}
In this case, we will explore two different methods for randomized rounding. One will work when $|C_i|$ is small and other when it is large. Here $|C_i|$ denotes the number of literals in $i^{th}$ clause $C_i$. For a given problem we will choose any one of the method randomly uniformly and prove that this process will give a good approximation ratio in expectation.\\
\noindent
{\bf Method 1}\\
Here, we will simply assign values from \{true,false\} to every variable randomly uniformly and independently of the others. Clearly,\[ p_{i,1} = Pr[C_i \,\, is \,\, satisfied] = 1 - 2^{-|C_i|} 
\]
Obviously, this method will produce good enough results only when individual $|C_i|$ are large enough.

\noindent
{\bf Method 2}\\
This method uses a technique called Arithmetization to convert the satisfiability problem to ILP problem. Given $m$ clauses each having variables from the set $\{x_i|i\in [n]\}$, let us define two sets for each clause: $P(i) = \{j|x_j$ appears unnegated in $C_i\}$ and $N(i) = \{j|x_j$ appears negated in $C_i\}$. Letting $z_i \in \{0,1\}$ we need to maximize $\sum w_iz_i$ subject to,
\begin{equation}
\forall i, z_i \leq \sum\limits_{j\in P(i)} x_j + \sum\limits_{j\in N(i)} (1-x_j)
\label{constraint}
\end{equation}

Here, $x_j$ is set to 1 if the corresponding boolean variable is set to true in the Max-SAT problem and 0 otherwise.

Again applying LP relaxation (allowing $z_i \in [0,1]$), we obtain the solution in the form of $x_j^*$ and $z_j^*$. Letting the objective function value for this LP problem be $y^*$, we have $y^*$ as an upper bound on $OPT$ which is the optimum value for the objective function in the original ILP problem.

To map the LP solution to the original problem, we will assign $x_j=1$ with probability $x_j^*$.

Without loss of generality assuming that all the variables appearing in $C_i$ are non-negated, from $z_i \in [0,1] \Rightarrow z_i^* \leq 1 $ and equation \ref{constraint} we get:
\begin{equation}
z_i^* = min \left\{ \sum\limits_{j \in P(i)}x_j^*,1 \right\}
\label{min}
\end{equation}
We have, $Pr[C_i$ is satisfied$] = 1 - \prod_{j\in P(i)}(1-x_j^*)$. From equation \ref{min}, it is clear that this probability will be minimum when all $x_j^*$'s are set to the same value (From the A.M. $\geq$ G.M. inequality). From equation \ref{min} this minimum value can be attained when $x_j^* = z_i^*/|C_i|$. Thus, we get:
\begin{equation}
p_{i,2} = Pr(C_i\,\, is\,\, satisfied) \geq 1 - (1-z_i^*/|C_i|)^{|C_i|}
\label{pi2}
\end{equation}
Clearly, for a fixed $z_i^*$,$ p_{i,2}$ decreases monotonically as $|C_i|$ increases. 

Now, we need to choose one of the two methods randomly uniformly and analyse the how well this process does. We get,
\[
\begin{array}{rcl}
Pr(C_i\,\, is\,\, satisfied) & = & (p_{i,1} + p_{i,2})/2\\
	& \geq & 1 - \frac {\left( 2^{-|C_i|} + \left( 1 - \frac{z_i^*}{|C_i|} \right)^{|C_i|}\right)}{2} = G \\
\end{array}
\]

To find a lower bound for $G$ consider the function:
\[
\begin{array}{rcll}
f(l,x) & = & 1 - \frac {\left( 2^{-l} + \left( 1 - \frac{x}{l} \right)^{l}\right)}{2} - \frac{3x}{4} & \\

\frac{\partial f(l,x)}{\partial x} & = & \frac{\left( 1 - \frac{x}{l} \right)^{l-1}}{2} - \frac{3}{4} &\\
		& \leq & \frac{1}{2} - \frac{3}{4} & $ Putting $x=0\\
\therefore \frac{\partial f(l,x)}{\partial x} & \leq & 0 & \forall x \in [0,1]
\end{array}
\]
Thus, $f(l,x)$ decreases with as $x \in [0,1]$ increases. Hence, its minimum value must occur at $x=1$. We have, $f(1,1) = f(2,1) = 0$ and for $l\geq 3\,, 2^{-l} \leq 1/8$ and $(1-1/l)^l \leq 1/e$. Hence, $f(l,1) \geq 1- \frac{1}{16} - \frac{1}{2e} - \frac{3}{4} \geq 0 \,\, \forall l \in \mathbb{Z}$. Since minimum value of $f(l,x)$ is also greater than 0 for a fixed $l$, $f(l,x) \geq 0 \,\, \forall x$ and therefore $G \geq \frac{3}{4}z^*$.

Thus, by linearity of expectation, expected value of objective function for reported solution = $\sum_i (3/4)z^* = (3/4)y^* \geq (3/4)OPT$.

It can also be observed that the bound achieved by this analysis cannot be improved. If we have all the 4 possible clauses (having equal weights) in 2 variables $x_1$ and $x_2$, any setting of values to $x_1$ and $x_2$ will give $OPT = 3$, but setting all of them to 0.5 will give $y^* = 4$. Hence, there are situations where $OPT = (3/4)y^*$. This means if we guaranteed $y_{rep} \geq f(y^*) > (3/4)y^*$, it will fail in this case since $y_{rep}$ is always $\leq OPT (= (3/4) y^*$ in this case).
